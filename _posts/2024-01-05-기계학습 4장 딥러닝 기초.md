# 4장 딥러닝 기초

### 들어가는 말

MLP가 등장하여 혁신을 이루던 1980년대 이미 깊은 신경망에 대한 아이디어는 나왔었다. 하지만 층을 늘릴수록 가중치가 소실되는 문제가 있었다. 이에 다양한 연구가 진행되고 1990년, 2000년에 걸쳐 딥러닝에 대한 긍정적인 연구 결과가 발표되었다.

### 딥러닝 기술의 혁신 요인

- 컨볼루션 신경망이 딥러닝의 가능성을 열었다.
- 값싼 GPU가 등장하였다.
- 인터넷 덕분에 학습 데이터가 크게 늘어났다.
- 계산은 단순한데 성능은 더 좋은 활성함수가 개발되었다.
- 학습에 효과적인 다양한 규제 기법이 개발되었다.
- 층별 예비학습 기법이 개발되었다.

### 특징 학습의 부각

기계학습의 적용 범위는 분류, 회귀로 제한되어 있었고 특징 추출은 수작업으로 구현한 후에 결합하여 사용했다. 하지만 딥러닝은 특징을 추출이라는 작업 자체도 학습으로 수행한다. 현대 기계 학습에서는 특징을 자동 학습하는 일을 무척 중요하게 취급하고, 특징 학습(feature training) 또는 표현 학습(representation learning)이라는 용어를 많이 사용한다.

### 깊은 다층 퍼셉트론

![Untitled](https://github.com/gyuilLim/gyuilLim.github.io/assets/50009192/7758e316-e4be-457c-a4bb-d41d82d35245)

입력 특징 : $\mathbf{x} = (1,x_1,x_2,\cdots,x_d)^T$

출력값 : $\mathbf{o} = (o_1, o_2,\cdots,o_c)^T$

$l$번째 층의 노드 개수 : $n_l$

가중치 행렬 : $U^l$

$$
U^l = \begin{bmatrix}u^l_{00}, u^l_{01}\cdots u^l_{0n_l-1}\\
u^l_{10},u^l_{11} \cdots u^l_{1n_l-1}\\
\vdots \\
u^l_{n_l0},u^l_{n_l1} \cdots u^l_{n_ln_l-1} \end{bmatrix}
$$

위 그림의 전방 계산을 함수로 매핑하여 나타내면 다음과 같다.

$$
\mathbf o = f(\mathbf x) = f_L(\cdots f_2(f_1(x)))
$$

함수 $f$는 함수 입력 벡터 $\mathbf x$를 출력값 $\mathbf o$로 매핑하는 함수이다. $f_L$ 은 $l$ 번째 층에서 입력값을 출력값으로 매핑하는 함수이다.

다음으로 $l$번째 은닉층의 $j$ 번째 노드의 연산은 다음과 같다.

$$
z^l_j = \tau_l(s^l_j)\\
s^l_j = \mathbf u^l_j \mathbf z^{l-1}\\
\mathbf z^{l-1} = (1,z^{l-1}_1, z^{l-1}_2, \cdots, z^{l-1}_{n_{l-1}})^T, \mathbf u^l_j = (u^l_{j0}, u^l_{j1}, \cdots, u^l_{jn-1})
$$

은닉층에 해당하는 $l=1,2,\dots,L-1$ 번째 층의 활성함수 $\tau_1 \sim \tau_{L-1}$ 로는 주로 ReLU 함수를 사용하고 마지막 출력층의 활성함수로는 로지스틱 시그모이드, tanh, softmax 등을 사용한다. 위 수식은 l번째 층의 j번째 노드의 연산만 표현하고 있는데, l번째 층에 있는 모든 노드의 연산으로 확장하면 다음과 같이 나타낼 수 있다.

$$
\mathbf z^l =\mathbf \tau_l(\mathbf U^l\mathbf z^{l-1}), 1 \le l \le L
$$

$\mathbf z^{l-1}$은 l-1번째 층 노드들의 출력값을 의미하고 이 출력값들과 l-1번째에서 l번째 층으로 향하는 가중치 행렬인 $\mathbf U^l$ 을 행렬곱하여 활성함수를 통과시키면 l번째 층의 출력값을 의미하는 $\mathbf z^l$ 을 내보내게 된다.

### 오류 역전파 알고리즘 - 출력층(L)에서 은닉층(1~L-1)까지

DMLP가 아닌 MLP에서 출력층으로부터 은닉층까지의 오류 역전파는 다음의 수식으로 유도되었다.

$$
\delta_k = (y_k - o_k)\tau'(osum_k), 1 \le k \le c \\
\frac {\partial J}{\partial u^2_{kj}} = \vartriangle u^2_{kj} = -\delta_k z_j, 0 \le j \le p, 1 \le k \le c
$$

위 수식은 은닉층의 j번째 노드에서 출력층의 k번째 노드로 향하는 가중치를 업데이트하는 수식이다. $\delta_k$ 는 k번째 출력값과 타겟값의 차이, 출력층에서의 활성함수 미분값이 곱해져있다. 즉 출력층에 입력되는 입력값의 오류율이라고 볼 수 있다. 여기에 은닉층의 j번째 노드의 출력값 $z_j$을 곱해주어 은닉층의 j번째 노드에서 출력층의 j번째 노드로 향하는 가중치의 그래디언트를 계산할 수 있다. 위 식을 DMLP로 확장하여 다시 쓰면 다음과 같이 나타낼 수 있다.

$$
\delta^L_k = \tau'_L(s^L_k)(y_k-o_k), 1 \le k \le c \\
\frac{\partial J}{\partial u^L_{kr}} = -\delta^L_k z^{L-1}_r, 0\le r \le n_{L-1}, 1 \le k \le c
$$

L번째 층은 출력층을 의미한다. $\delta^L_k$는 출력층에 있는 k번째 노드에 입력되는 입력값의 오류율을 의미한다. 여기에 L-1번째 층의  r번째 노드에서의 출력값을 의미하는 $z^{l-1}_r$을 곱해주어 L-1층의 r번째 노드에서 L층의 k번째 노드로 향하는 가중치의 그래디언트를 계산할 수 있다. 출력층으로부터 은닉층까지의 그래디언트를 계산하는 것은 DMLP와 MLP 둘 다 유사하다. 결국 마지막 층 1개의 가중치 그래디언트를 계산하는 것이기 때문이다.

### 오류 역전파 알고리즘 - 은닉층(1~L-1)에서 입력층(0)까지

먼저 MLP에서는 은닉층으로부터 입력층까지에 대한 오류 역전파를 다음 수식과 같이 유도하였다.

$$
\eta_j = \tau'(zsum_j)\sum_{q=1}^c \delta_q u^2_{qj}, 1 \le j \le p \\
\frac{\partial J}{\partial u^1_{ji}} = \vartriangle u^1_{ji}=-\eta_jx_i, 0 \le i \le d, 1 \le j \le p
$$

$\eta_j$ 는 은닉층 j번째 노드에서 입력값의 오류율을 의미한다. 여기에 입력층의 i번째 노드의 입력값인 $x_i$를 곱해주어 입력층의 i번째 노드에서 은닉층의 j번째 노드로 향하는 가중치의 그래디언트를 계산한 것이다. 위 식을 DMLP로 확장하여 다시 쓰면 다음과 같이 나타낼 수 있다.

$$
\delta^l_j = \tau'_l(s^l_j) \sum_{p=1}^{n_{j+1}}\delta^{i+1}_p u^{l+1}_{l}, 1 \le j \le n_i \\
\frac{\partial J}{\partial u^l_{ji}}=-\delta^l_jz^{l-1}_i, 0 \le i \le n_{l-1}, 1 \le j \le n_i
$$

MLP의 수식에서 층과 관련된 인덱스만 추가한 식이다. 먼저 $\delta^l_j$는 l번째 은닉층의 j번째 노드에서 입력값의 오류율을 의미한다. 여기에 l-1번째 은닉층의 i번째 노드의 출력값인 $z^{l-1}_i$를 곱해주어 l-1번째 은닉층의 i번째 노드에서 l번째 은닉층의 j번째 노드로 향하는 가중치의 그래디언트를 계산할 수 있다.

### ResNet

![Untitled](https://github.com/gyuilLim/gyuilLim.github.io/assets/50009192/0272020e-b63c-4da3-973b-a504c88f9813)

그림(a)에서 입력 텐서 $\mathbf x$는 컨볼루션층 2개를 거쳐 $\mathbf f(\mathbf x)$가 된다. 이 과정은 $\mathbf F(\mathbf x) = \tau(\mathbf x \circledast w_1) \circledast w_2$  와 같은 수식으로 표현할 수 있다. 이 계산 결과에 shortcut으로 연결된 입력 텐서 $\mathbf x$를 더해주고 활성함수 ReLU까지 통과하는 과정을 식으로 나타내면 다음과 같다.

$$
y = \tau(F(\mathbf x) + \mathbf x)
$$

shortcut 구조를 사용한 이점은 뭘까? [He2016b]에 나와있는 이유는 다음과 같다.

$$
\mathbf y_l = h(\mathbf x_l) + F(\mathbf x_l, W_l), \\
x_{l+1} = f(\mathbf y_l)
$$

위 수식에서 $h(x_l) = x_l$로 항등함수를 의미한다. $F$는 입력텐서 x에 컨볼루션 연산을 적용한 출력값을 의미하고, $f$는 활성함수 ReLU이다. 여기서 활성함수 $f$도 항등함수라고 가정한다면 $x_{l+1} = y_l$이 되기 때문에

$$
\mathbf x_{l+1} = \mathbf x_l + F(\mathbf x_l, W_l)
$$

위와 같이 식을 정리할 수 있게 되었다. 위 수열을 일반화 하기위해 전개 양상을 확인해보자.

$$
\mathbf x_{l+2} = \mathbf x_{l+1} + F(\mathbf x_{l+1}) = \mathbf x_l + F(\mathbf x_{l}) + F(\mathbf x_{l+1})\\
\mathbf x_L = \mathbf x_l + \sum_{i=1}^{L-1}F(\mathbf x_i)
$$

최종적으로 위 수식으로 정리할 수 있게 된다. 이 때 L > l이다. 즉 L은 l보다 깊은 층을 의미한다.  마지막으로 임의의 목적함수 $\varepsilon$를 정의했다고 하자. $\mathbf x_l$의 변화량을 나타내기 위해 목적함수를 $\mathbf x_l$에 대하여 편미분하면

$$
\frac{\partial \varepsilon}{\partial \mathbf x_l} = \frac{\partial \varepsilon}{\partial \mathbf x_L} \frac{\partial \mathbf x_L}{\partial \mathbf x_l} = \frac{\partial \varepsilon}{\partial \mathbf x_L} (1 + \frac{\partial \sum_{i=1}^{L-1}F(\mathbf x_i)}{\partial \mathbf x_l})
$$

위와 같은 수식으로 나타낼 수 있다. 이 때 $\frac{\partial \sum_{i=1}^{L-1}F(\mathbf x_i)}{\partial \mathbf x_l}$ 항은 -1이 될 가능성이 거의 없기때문에 마찬가지로 $1 + \frac{\partial \sum_{i=1}^{L-1}F(\mathbf x_i)}{\partial \mathbf x_l}$은 0이 되지 않아 그래디언트 소실 문제가 발생할 가능성이 현저히 줄어들게 되어 층 깊이에 자유로워지는 것이다. 만약 shortcut을 추가하지 않았다면 위 수식에서 상수 1이 생기지 않아 $\frac{\partial \sum_{i=1}^{L-1}F(\mathbf x_i)}{\partial \mathbf x_l}$이 0으로 수렴해버리는 문제를 해결했다는 것으로 볼 수 있다.